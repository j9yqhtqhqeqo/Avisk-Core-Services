{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import Sequential, layers, optimizers\n",
    "from keras import regularizers\n",
    "from keras_tuner import Hyperband\n",
    "\n",
    "# columns_to_load = [\n",
    "#     \"Sector_EI\",\n",
    "#     \"Compnay_EI\",  # Will rename to \"Company_EI\" after loading\n",
    "#     \"Sector_EM\",\n",
    "#     \"Company_EM\",\n",
    "#     \"Sector_IM\",\n",
    "#     \"Company_IM\",\n",
    "#     \"sector_exposure_id\",\n",
    "#     \"Normalized_Stock_Price_Change\"\n",
    "# ]\n",
    "\n",
    "columns_to_load = [\n",
    "    \"Sector_EI\",\n",
    "    \"Compnay_EI\",\n",
    "    \"sector_exposure_id\",\n",
    "    \"Normalized_Stock_Price_Change\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/NN_Data_Input/X_Train_Stck_Price_Chg_Pct_Pos.csv'  # Replace with your file path\n",
    "\n",
    "# file_path = '/Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/NN_Data_Input/X_Train_Stck_Price_Chg_Pct_Full.csv'  # Replace with your file path\n",
    "\n",
    "\n",
    "\n",
    "risk_dataset = pd.read_csv(file_path, usecols=columns_to_load)\n",
    "\n",
    "# Rename the column \"Compnay_EI\" to \"Company_EI\"\n",
    "risk_dataset.rename(columns={\"Compnay_EI\": \"Company_EI\"}, inplace=True)\n",
    "\n",
    "X = risk_dataset.drop('Normalized_Stock_Price_Change', axis=1)\n",
    "y = risk_dataset['Normalized_Stock_Price_Change']\n",
    "\n",
    "# Standardize the features\n",
    "standard_scalar = StandardScaler()\n",
    "X_scaled = standard_scalar.fit_transform(X)\n",
    "# X_scaled = X\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.33, random_state=33)\n",
    "\n",
    "# Define the model-building function for hyperparameter tuning\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(3,)))  # Adjust input shape based on dataset\n",
    "\n",
    "    # Tune the number of layers and units per layer\n",
    "    for i in range(hp.Int('num_layers', 2, 4)):  # 2 to 4 hidden layers\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=64,\n",
    "                         max_value=256, step=64),  # Units per layer\n",
    "            # Activation function\n",
    "            activation=hp.Choice('activation', ['relu', 'tanh']),\n",
    "            # L2 Regularization to avoid overfitting\n",
    "            kernel_regularizer=regularizers.l2(0.01)\n",
    "        ))\n",
    "\n",
    "        # Optional: Add Dropout for regularization\n",
    "        # Dropout with 20% probability to avoid overfitting\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Output layer with 1 neuron for regression\n",
    "    # Single output for regression\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model with tunable learning rate\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=hp.Choice(\n",
    "            'learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the tuner (Hyperband)\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_mae',  # Minimize validation MAE\n",
    "    max_epochs=50,        # Maximum number of epochs for tuning\n",
    "    factor=3,             # Factor for reducing the range of hyperparameters\n",
    "    directory='hyperparam_tuning',  # Directory to store the results\n",
    "    project_name='risk_prediction_optimized'\n",
    ")\n",
    "\n",
    "# Start the search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, validation_split=0.2, epochs=25, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')}.\n",
    "The optimal units in each layer are {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}.\n",
    "The optimal activation function is {best_hps.get('activation')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the best model with the selected hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the entire training dataset\n",
    "history = best_model.fit(\n",
    "    X_train, y_train, validation_split=0.2, epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_mae = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Plot the training history (optional)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497, 5)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "[0.3245357  0.10762902 0.11390446 0.18155478 0.12672329 0.09530315\n",
      " 0.17777896 0.12447013 0.09530315 0.09530315]\n",
      "1.4425058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# columns_to_load = [\n",
    "#     \"company_name\",\n",
    "#     \"year\",\n",
    "#     \"Sector_EI\",\n",
    "#     \"Compnay_EI\",  # Will rename to \"Company_EI\" after loading\n",
    "#     \"Sector_EM\",\n",
    "#     \"Company_EM\",\n",
    "#     \"Sector_IM\",\n",
    "#     \"Company_IM\",\n",
    "#     \"sector_exposure_id\"\n",
    "# ]\n",
    "\n",
    "columns_to_load = [\n",
    "    \"company_name\",\n",
    "    \"year\",\n",
    "    \"Sector_EI\",\n",
    "    \"Compnay_EI\",\n",
    "    \"sector_exposure_id\"\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "test_file_path = '/Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/NN_Data_Input/X_Train_Stck_Price_Chg_Pct_Pos.csv'  # Replace with your file path\n",
    "df = pd.read_csv(test_file_path, usecols=columns_to_load)\n",
    "\n",
    "print(df.shape)\n",
    "# Rename the column \"Compnay_EI\" to \"Company_EI\"\n",
    "df.rename(columns={\"Compnay_EI\": \"Company_EI\"}, inplace=True)\n",
    "\n",
    "company_filter = df[\"company_name\"] == \"PIONEER NATURAL RESOURCES\"\n",
    "year_filetr = df[\"year\"] == 2013\n",
    "sample_df1 = df.where(company_filter).dropna()\n",
    "sample_df2 = sample_df1.where(year_filetr).dropna()\n",
    "data_input_nn = sample_df2.drop('company_name', axis=1).drop('year', axis=1)\n",
    "# print(data_input_nn)\n",
    "\n",
    "standard_scalar = StandardScaler()\n",
    "X_scaled = standard_scalar.fit_transform(data_input_nn)\n",
    "\n",
    "predicted_values = best_model.predict(X_scaled)\n",
    "print(predicted_values.flatten())\n",
    "\n",
    "print(predicted_values.flatten().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,461</span> (201.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,461\u001b[0m (201.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,153</span> (67.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,153\u001b[0m (67.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,308</span> (134.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m34,308\u001b[0m (134.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your MobileNet model\n",
    "for layer in best_model.layers:\n",
    "    print(f\"Layer Name: {layer.name}\")\n",
    "    weights = layer.get_weights()  # Get weights and biases\n",
    "    if weights:  # Some layers might not have weights\n",
    "        print(\"Weights:\", weights[0])  # Kernel weights\n",
    "        print(\"Biases:\", weights[1] if len(weights) > 1 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          Normalized_Stock_Price_Change  \\\n",
      "company_name          sector_exposure_path_name                                                           \n",
      "ANTERO RESOURCES Corp Associated Energy Costs                                                 14.461116   \n",
      "                      Dividend/reinvestment balance for long-term growth                       7.882282   \n",
      "                      GDP and Disposable Income                                               17.955125   \n",
      "                      Innovation                                                              18.720362   \n",
      "                      Market Transition                                                       17.958664   \n",
      "...                                                                                                 ...   \n",
      "VITAL ENERGY, INC.    Professional Skills Development                                        -11.941073   \n",
      "                      Provisioning services (Raw Materials, Nutrients...                     -27.571233   \n",
      "                      Regulated Carbon Price                                                 -12.986367   \n",
      "                      Wages and Benefits                                                     -10.528111   \n",
      "                      Well-being and health                                                   -8.963004   \n",
      "\n",
      "                                                                          Exp_Pct_Chg  \n",
      "company_name          sector_exposure_path_name                                        \n",
      "ANTERO RESOURCES Corp Associated Energy Costs                                0.320709  \n",
      "                      Dividend/reinvestment balance for long-term growth     0.894463  \n",
      "                      GDP and Disposable Income                              0.623181  \n",
      "                      Innovation                                             0.494428  \n",
      "                      Market Transition                                      0.626999  \n",
      "...                                                                               ...  \n",
      "VITAL ENERGY, INC.    Professional Skills Development                        0.323708  \n",
      "                      Provisioning services (Raw Materials, Nutrients...     0.174519  \n",
      "                      Regulated Carbon Price                                 0.282119  \n",
      "                      Wages and Benefits                                     0.354444  \n",
      "                      Well-being and health                                  1.221231  \n",
      "\n",
      "[190 rows x 2 columns]\n",
      "Updated file saved to /Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/NN_Data_Input/Exp_PCT_CHG_Summarized.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data (assuming the data is in a CSV file)\n",
    "# Replace 'file.csv' with the actual filename\n",
    "file_path = '/Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/NN_Data_Input/Exp_PCT_CHG_Updated.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Assuming your dataframe is named df\n",
    "# First, sort the dataframe by company_name, year, and sector_exposure_path_name\n",
    "df = df.sort_values(by=['company_name', 'year', 'sector_exposure_path_name'])\n",
    "\n",
    "# Now, we need to calculate pct_change for each group of company_name and sector_exposure_path_name\n",
    "grouped_df = df.groupby(['company_name', 'sector_exposure_path_name'])[[\n",
    "    'Normalized_Stock_Price_Change','Exp_Pct_Chg']].sum()\n",
    "\n",
    "# The pct_change method calculates the percentage change from the prior row, so no need to manually implement the formula.\n",
    "# pct_change automatically calculates: (current - prior) / prior * 100\n",
    "\n",
    "# Optional: Convert the pct_change to percentage format by multiplying by 100, if you need it as a percentage (not a decimal)\n",
    "# df['Exp_Pct_Chg'] = df['Exp_Pct_Chg'] * 100\n",
    "\n",
    "# If you want to see the updated dataframe:\n",
    "print(grouped_df)\n",
    "\n",
    "# Save the updated dataset\n",
    "output_file_path = '/Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/NN_Data_Input/Exp_PCT_CHG_Summarized.csv'\n",
    "grouped_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated file saved to {output_file_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
