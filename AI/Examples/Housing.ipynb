{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, layers, optimizers\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m regularizers\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_tuner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hyperband\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import Sequential, layers, optimizers\n",
    "from keras import regularizers\n",
    "from keras_tuner import Hyperband\n",
    "\n",
    "# Load your dataset\n",
    "\n",
    "housing_dataset = pd.read_csv(\n",
    "    '/Users/mohanganadal/Data Company/Text Processing/Programs/DocumentProcessor/Source Code/Data-Company/AI/HousingData.csv')\n",
    "\n",
    "housing_dataset = housing_dataset.rename(\n",
    "    columns={'MEDV': 'Price'})  # Rename target column if needed\n",
    "housing_dataset = housing_dataset.fillna(0)\n",
    "\n",
    "X = housing_dataset.drop('Price', axis=1)\n",
    "y = housing_dataset['Price']\n",
    "\n",
    "# Standardize the features\n",
    "standard_scalar = StandardScaler()\n",
    "X_scaled = standard_scalar.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.33, random_state=33)\n",
    "\n",
    "# Define the model-building function for hyperparameter tuning\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(13,)))  # Adjust input shape based on dataset\n",
    "\n",
    "    # Tune the number of layers and units per layer\n",
    "    for i in range(hp.Int('num_layers', 2, 4)):  # 2 to 4 hidden layers\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=64,\n",
    "                         max_value=256, step=64),  # Units per layer\n",
    "            # Activation function\n",
    "            activation=hp.Choice('activation', ['relu', 'tanh']),\n",
    "            # L2 Regularization to avoid overfitting\n",
    "            kernel_regularizer=regularizers.l2(0.01)\n",
    "        ))\n",
    "\n",
    "        # Optional: Add Dropout for regularization\n",
    "        # Dropout with 20% probability to avoid overfitting\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Output layer with 1 neuron for regression\n",
    "    # Single output for regression\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model with tunable learning rate\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=hp.Choice(\n",
    "            'learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the tuner (Hyperband)\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_mae',  # Minimize validation MAE\n",
    "    max_epochs=50,        # Maximum number of epochs for tuning\n",
    "    factor=3,             # Factor for reducing the range of hyperparameters\n",
    "    directory='hyperparam_tuning',  # Directory to store the results\n",
    "    project_name='housing_price_prediction_optimized'\n",
    ")\n",
    "\n",
    "# Start the search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')}.\n",
    "The optimal units in each layer are {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}.\n",
    "The optimal activation function is {best_hps.get('activation')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the best model with the selected hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the entire training dataset\n",
    "history = best_model.fit(\n",
    "    X_train, y_train, validation_split=0.2, epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_mae = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Plot the training history (optional)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(loss)+1)\n",
    "# Plot training and validation loss\n",
    "plt.plot(epochs,loss,'y',label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Predict values for the top 10 samples in the training set\n",
    "top_10_samples = X_train[:10]  # Extract the first 10 samples\n",
    "predicted_values = best_model.predict(top_10_samples)\n",
    "\n",
    "# Extract the corresponding actual values from y_train\n",
    "actual_values = y_train[:10].to_numpy() if isinstance(\n",
    "    y_train, pd.Series) else y_train[:10]\n",
    "\n",
    "\n",
    "# Print the predictions and actual values side by side\n",
    "print(\"Predicted vs Actual values for the top 10 samples:\")\n",
    "print(f\"{'Sample':<10}{'Predicted':<15}{'Actual':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for i, (pred, actual) in enumerate(zip(predicted_values, actual_values), start=1):\n",
    "    print(f\"{i:<10}{pred[0]:<15.3f}{actual:<10.3f}\")\n",
    "\n",
    "actual_values = actual_values.flatten()\n",
    "predicted_values = predicted_values.flatten()\n",
    "\n",
    "# Compute errors\n",
    "absolute_errors = np.abs(predicted_values - actual_values)\n",
    "mae = mean_absolute_error(actual_values, predicted_values)\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print side-by-side comparison and errors\n",
    "print(\"Predicted vs Actual values and Errors:\")\n",
    "print(f\"{'Sample':<10}{'Predicted':<15}{'Actual':<10}{'Abs Error':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for i, (pred, actual, abs_err) in enumerate(zip(predicted_values, actual_values, absolute_errors), start=1):\n",
    "    print(f\"{i:<10}{pred:<15.3f}{actual:<10.3f}{abs_err:<15.3f}\")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nError Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
